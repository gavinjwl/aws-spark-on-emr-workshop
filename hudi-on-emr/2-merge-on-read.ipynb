{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Merge-on-Read (MoR)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup pyspark kernel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "        \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read sample data and clean"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "raw = spark.read.csv(\n",
    "    's3://aws-data-analytics-blog/emrimmersionday/tripdata.csv',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "raw.cache()\n",
    "\n",
    "raw.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "raw.show(1, vertical=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import (\n",
    "    unix_timestamp, col, from_unixtime,\n",
    "    year, month, dayofmonth, hour, row_number\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = raw.na.fill(\n",
    "        {'ehail_fee': '0'}\n",
    "    ).withColumn(\n",
    "        \"ehail_fee\", col(\"ehail_fee\").cast(\"int\")\n",
    "    ).withColumn(\n",
    "        'lpep_pickup_datetime', \n",
    "        from_unixtime(unix_timestamp(col('lpep_pickup_datetime'), 'M/d/yy H:mm'))\n",
    "    ).withColumn(\n",
    "        'lpep_dropoff_datetime', \n",
    "        from_unixtime(unix_timestamp(col('lpep_dropoff_datetime'), 'M/d/yy H:mm')) \n",
    "    ).withColumn(\n",
    "        'lpep_pickup_datetime', col('lpep_pickup_datetime').cast('timestamp')\n",
    "    ).withColumn(\n",
    "        'lpep_dropoff_datetime', col('lpep_dropoff_datetime').cast('timestamp')\n",
    "    ).withColumn(\n",
    "        '_year', year(col('lpep_pickup_datetime'))\n",
    "    ).withColumn(\n",
    "        '_month', month(col('lpep_pickup_datetime'))\n",
    "    ).withColumn(\n",
    "        '_day', dayofmonth(col('lpep_pickup_datetime'))\n",
    "    ).withColumn(\n",
    "        '_hour', hour(col('lpep_pickup_datetime'))\n",
    "    ).orderBy(\n",
    "        ['lpep_pickup_datetime', 'lpep_dropoff_datetime']\n",
    "    ).withColumn(\n",
    "        \"id\", row_number().over(Window().orderBy(['lpep_pickup_datetime', 'lpep_dropoff_datetime']))\n",
    "    )\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.select(\n",
    "    'id', 'VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', '_year', '_month', '_day', '_hour', 'ehail_fee',\n",
    "    'passenger_count', 'trip_distance'\n",
    ").show(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly = df.select('_hour').distinct().orderBy('_hour').collect()\n",
    "hourly"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_df = [\n",
    "    df.where(f'_hour = {row._hour}')\n",
    "    for row in hourly\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create MoR table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "empty_df = spark.createDataFrame(\n",
    "    spark.sparkContext.emptyRDD(),\n",
    "    df.schema\n",
    ")\n",
    "\n",
    "empty_df.printSchema()\n",
    "\n",
    "empty_df.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "database_name = 'default'\n",
    "table_name = 'mor_tripdata'\n",
    "table_type = 'MERGE_ON_READ'\n",
    "\n",
    "bucket_name = ''\n",
    "base_path = f's3://{bucket_name}/hudi/{table_name}'\n",
    "\n",
    "# hoodie options\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': table_name,\n",
    "    'hoodie.table.type': table_type,\n",
    "    'hoodie.metadata.enable': 'true',\n",
    "    \n",
    "    'hoodie.compact.inline.max.delta.seconds': 3600,      # Default Value: 3600, since v0.9.0\n",
    "    'hoodie.compact.inline.max.delta.commits': 10,        # Default Value: 10\n",
    "\n",
    "    'hoodie.cleaner.delete.bootstrap.base.file': 'false', # Default Value: false, since v0.9.0\n",
    "    'hoodie.cleaner.commits.retained': 10,                # Default Value: 10\n",
    "    'hoodie.commits.archival.batch': 10,                  # Default Value: 10\n",
    "\n",
    "    'hoodie.datasource.write.table.name': table_name,\n",
    "    'hoodie.datasource.write.table.type': table_type,\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': '_year,_month,_day',\n",
    "    'hoodie.datasource.write.precombine.field': 'lpep_pickup_datetime',\n",
    "    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.ComplexKeyGenerator',\n",
    "    'hoodie.datasource.write.hive_style_partitioning': 'true',\n",
    "\n",
    "    'hoodie.parquet.small.file.limit': 104857600,        # default: 104857600 Bytes (100 MB)\n",
    "    'hoodie.parquet.max.file.size': 125829120,           # default: 125829120 Bytes (120 MB)\n",
    "    'hoodie.parquet.block.size': 125829120,              # default: 125829120 Bytes (120 MB)\n",
    "    'hoodie.parquet.page.size': 1048576,                 # default: 1048576 Bytes (1 MB)\n",
    "    'hoodie.parquet.compression.codec': 'snappy',\n",
    "\n",
    "    'hoodie.datasource.hive_sync.enable': 'true',\n",
    "    'hoodie.datasource.hive_sync.database': database_name,\n",
    "    'hoodie.datasource.hive_sync.table': table_name,\n",
    "    'hoodie.datasource.hive_sync.partition_fields': '_year,_month,_day',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "\n",
    "    'hoodie.insert.shuffle.parallelism': 2,\n",
    "    'hoodie.upsert.shuffle.parallelism': 2,\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': 2,\n",
    "    'hoodie.delete.shuffle.parallelism': 2,\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "empty_df.write.format('hudi')\\\n",
    "    .options(**hudi_options)\\\n",
    "    .option('hoodie.datasource.write.operation', 'insert')\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_df[0].write.format('hudi')\\\n",
    "    .options(**hudi_options)\\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert')\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_df[1].write.format('hudi')\\\n",
    "    .options(**hudi_options)\\\n",
    "    .option('hoodie.datasource.write.operation', 'insert')\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for _df in hourly_df[2:]:\n",
    "    _df.write.format('hudi')\\\n",
    "        .options(**hudi_options)\\\n",
    "        .option('hoodie.datasource.write.operation', 'insert')\\\n",
    "        .mode(\"append\")\\\n",
    "        .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read snapshot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tripsSnapshotDF = spark.read.format(\"hudi\")\\\n",
    "    .load(f'{base_path}')\n",
    "\n",
    "print(tripsSnapshotDF.count())\n",
    "\n",
    "tripsSnapshotDF.printSchema()\n",
    "\n",
    "tripsSnapshotDF\\\n",
    "    .orderBy('id')\\\n",
    "    .select('_hoodie_commit_time','_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name')\\\n",
    "    .show(3, vertical=True, truncate=False)\n",
    "\n",
    "tripsSnapshotDF\\\n",
    "    .orderBy('id', ascending=False)\\\n",
    "    .select('_hoodie_commit_time','_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name')\\\n",
    "    .show(3, vertical=True, truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Incremental query"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rows = tripsSnapshotDF\\\n",
    "    .select('_hoodie_commit_time')\\\n",
    "    .distinct()\\\n",
    "    .orderBy('_hoodie_commit_time')\\\n",
    "    .limit(50)\\\n",
    "    .collect()\n",
    "\n",
    "commits = [row[0] for row in rows]\n",
    "print(commits, '\\n')\n",
    "\n",
    "beginTime = commits[len(commits) - 3]\n",
    "print(beginTime, '\\n')\n",
    "\n",
    "# incrementally query data\n",
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': beginTime,\n",
    "}\n",
    "\n",
    "incremental_df = spark.read.format(\"hudi\")\\\n",
    "    .options(**incremental_read_options)\\\n",
    "    .load(f'{base_path}')\n",
    "\n",
    "print(incremental_df.count(), '\\n')\n",
    "\n",
    "incremental_df.select('_hoodie_commit_time','_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name')\\\n",
    "    .orderBy('_hoodie_commit_time')\\\n",
    "    .show(3, vertical=True, truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Point in time query"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "beginTime = \"000\" # Represents all commits > this time.\n",
    "endTime = commits[len(commits) - 3]\n",
    "\n",
    "# query point in time data\n",
    "point_in_time_read_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.begin.instanttime': beginTime,\n",
    "    'hoodie.datasource.read.end.instanttime': endTime,\n",
    "}\n",
    "\n",
    "incremental_df = spark.read.format(\"hudi\")\\\n",
    "    .options(**point_in_time_read_options)\\\n",
    "    .load(f'{base_path}')\n",
    "\n",
    "print(incremental_df.count(), '\\n')\n",
    "\n",
    "incremental_df.select('_hoodie_commit_time','_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name')\\\n",
    "    .orderBy('_hoodie_commit_time', ascending=False)\\\n",
    "    .show(3, vertical=True, truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Update data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "hourly_df[0].withColumn('passenger_count', lit(-1))\\\n",
    "    .write.format('hudi')\\\n",
    "    .options(**hudi_options)\\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert')\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(base_path)\n",
    "\n",
    "spark.read.format(\"hudi\")\\\n",
    "    .load(f'{base_path}')\\\n",
    "    .select(\n",
    "        '_hoodie_commit_time','_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name',\n",
    "        'id', 'passenger_count'\n",
    "    ).show(3, vertical=True, truncate=False)\n",
    "\n",
    "spark.sql(\n",
    "f'''\n",
    "select count(1) as ro_count\n",
    "from {database_name}.{table_name}_ro\n",
    "''').show()\n",
    "\n",
    "spark.sql(\n",
    "f'''\n",
    "select count(1) as rt_count\n",
    "from {database_name}.{table_name}_rt\n",
    "''').show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "to_be_delete = df.where('VendorID = 1')\\\n",
    "    .select('id', '_year', '_month', '_day')\\\n",
    "    .withColumn('lpep_pickup_datetime', lit(0.0))\n",
    "print(to_be_delete.count())\n",
    "\n",
    "to_be_delete.write.format(\"hudi\")\\\n",
    "    .options(**hudi_options)\\\n",
    "    .option('hoodie.datasource.write.operation', 'delete')\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(f'{base_path}')\n",
    "\n",
    "spark.read.format(\"hudi\")\\\n",
    "    .load(f'{base_path}')\\\n",
    "    .select(\n",
    "        '_hoodie_commit_time','_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name',\n",
    "        'id', 'VendorID'\n",
    "    ).show(3, vertical=True, truncate=False)\n",
    "\n",
    "spark.sql(\n",
    "f'''\n",
    "select count(1) as ro_count\n",
    "from {database_name}.{table_name}_ro\n",
    "''').show()\n",
    "\n",
    "spark.sql(\n",
    "f'''\n",
    "select count(1) as rt_count\n",
    "from {database_name}.{table_name}_rt\n",
    "''').show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "407910dc9194d5420ce0f0143fecb83fef8c348660603d21cda0898c7dd6e099"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}